{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2506c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hail\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ea8b5",
   "metadata": {},
   "source": [
    "# 1. Set Up Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38e77169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the HGDP+1kG dataset - after filtering, LD pruning and pca outliers removal \n",
    "mt = hl.read_matrix_table('gs://imary116/whole_dataset.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f168e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 16:45:57 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n",
      "  Loading field 'f3' as type str (user-supplied)\n",
      "  Loading field 'f4' as type str (user-supplied)\n",
      "  Loading field 'f5' as type str (not specified)\n"
     ]
    }
   ],
   "source": [
    "# read-in regions bed file \n",
    "bed_file = hl.import_bed('gs://schema2/data/variant_qc/3.1_hl_filter-genotypes-schema-gnomad/0.2.1/grch38.gencode.v29.p8.merged.merged_by_exonid.cds.protein_coding.bed', reference_genome='GRCh38')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "21cbaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter mt to bed file regions only\n",
    "mt_filt = mt.filter_rows(hl.is_defined(bed_file[mt.locus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4aaf3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the schema exome mt \n",
    "mt_schema = hl.read_matrix_table('gs://schema2/data/variant_qc/3.1_hl_filter-genotypes-schema-gnomad/0.2.1/schema2-gnomad.common.mt')\n",
    "\n",
    "# key the schema mt by locus only so the filtering can work\n",
    "mt_schema = mt_schema.key_rows_by(mt_schema.locus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "815f101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter filtered mt to schema shared variants \n",
    "mt_filt2 = mt_filt.filter_rows(hl.is_defined(mt_schema.rows()[mt_filt.locus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fcbb025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7318, 346787)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_schema.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a5518dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 16:47:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(325, 4097)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_filt2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d2593a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to combine two datasets, three requirements must be met:\n",
    "# 1. The row keys must match\n",
    "# 2. The column key schemas and column schemas must match.\n",
    "# 3. The entry schemas must match.\n",
    "\n",
    "# filtered ds \n",
    "mt_filt2_unkeyed = mt_filt2.key_cols_by().key_rows_by() # no key mt\n",
    "mt_filt2_clean = mt_filt2_unkeyed.select_cols(mt_filt2_unkeyed.s) # select the samples\n",
    "mt_filt2_clean = mt_filt2_clean.select_rows(mt_filt2_clean.locus, mt_filt2_clean.alleles) # select locus and alleles \n",
    "mt_filt2_clean = mt_filt2_clean.select_entries(mt_filt2_clean.GT) # select GT \n",
    "mt_filt2_clean = mt_filt2_clean.key_cols_by('s').key_rows_by(*['locus', 'alleles']) # put back the keys\n",
    "\n",
    "\n",
    "# schema ds \n",
    "mt_schema_unkeyed = mt_schema.key_cols_by().key_rows_by() # no key mt\n",
    "mt_schema_clean = mt_schema_unkeyed.select_cols(mt_schema_unkeyed.s) # select the samples\n",
    "mt_schema_clean = mt_schema_clean.select_rows(mt_schema_clean.locus, mt_schema_clean.alleles) # select locus and alleles \n",
    "mt_schema_clean = mt_schema_clean.select_entries(mt_schema_clean.GT) # select GT \n",
    "mt_schema_clean = mt_schema_clean.key_cols_by('s').key_rows_by(*['locus', 'alleles']) # put back the keys\n",
    "\n",
    "\n",
    "# merge the two ds \n",
    "final_mt = mt_filt2_clean.union_cols(mt_schema_clean, row_join_type='outer') # include all rows from both mt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3067f49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 16:47:42 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7325, 350884)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mt.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9b93e",
   "metadata": {},
   "source": [
    "# 2. Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3656e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 16:52:13 Hail: INFO: ld_prune: running local pruning stage with max queue size of 765 variants\n",
      "2022-09-19 16:52:17 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2022-09-19 16:53:30 Hail: INFO: wrote table with 5491 rows in 14901 partitions to /tmp/WR9MtJg9gckVIG5zi2zDgb\n",
      "    Total size: 709.11 KiB\n",
      "    * Rows: 709.10 KiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (21.00 B)\n",
      "    * Largest partition:  6 rows (316.00 B)\n",
      "2022-09-19 16:53:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2022-09-19 17:05:51 Hail: INFO: Wrote all 172 blocks of 5491 x 350884 matrix with block size 4096.\n",
      "2022-09-19 18:24:42 Hail: INFO: Wrote all 172 blocks of 5491 x 350884 matrix with block size 4096.\n"
     ]
    }
   ],
   "source": [
    "# LD pruning - remove correlated variants \n",
    "pruned = hl.ld_prune(final_mt.GT, r2=0.1, bp_window_size=500000) #   \n",
    "final_mt_pruned = final_mt.filter_rows(hl.is_defined(pruned[final_mt.row_key])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate related and unrelated individuals \n",
    "\n",
    "# compute kinship statistic\n",
    "relatedness_ht = hl.pc_relate(final_mt_pruned.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by() \n",
    "\n",
    "# identify closely related individuals in pairs (list of sample IDs) \n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False) \n",
    "\n",
    "# subset filtered mt to samples that are NOT present in the list of related individuals  \n",
    "mt_unrel = final_mt_pruned.filter_cols(hl.is_defined(related_samples_to_remove[final_mt_pruned.col_key]), keep=False) \n",
    "\n",
    "# do the same as above but this time subset to samples that are present in the related-individuals list   \n",
    "mt_rel = final_mt_pruned.filter_cols(hl.is_defined(related_samples_to_remove[final_mt_pruned.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32426b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and projection functions\n",
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_path: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a data set\n",
    "    :param mt: data set to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_path: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_path + reg_name + '_scores.txt.bgz')  # save individual-level-genetic-region PCs\n",
    "    pca_loadings.write(out_path + reg_name + '_loadings.ht', overwrite)  # save PCA loadings\n",
    "\n",
    "    \n",
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_path: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space of unrelated samples \n",
    "    :param project_mt: matrix table of related samples to project  \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_path: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_path + reg_name + '_projected_scores.txt.bgz') # save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on the unrelated samples\n",
    "run_pca(mt_unrel, 'global', 'gs://imary116/pca/', False)  \n",
    "\n",
    "# read in the PCA loadings of the unrelated samples\n",
    "loadings = hl.read_table('gs://imary116/pca/global_loadings.ht') \n",
    "\n",
    "# project the related samples onto the unrelated-samples' PC space \n",
    "project_individuals(loadings, mt_rel, 'global', 'gs://imary116/pca/', False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a791dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}